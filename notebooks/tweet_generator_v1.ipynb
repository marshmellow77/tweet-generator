{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Iran, the Number One State of Sponsored Terror with numerous violations of Human Rights occurring on an hourly basis, has now closed down the Internet so that peaceful demonstrators cannot communicate. Not good! HAPPY NEW YEAR! We are MAKING AMERICA GREAT AGAIN, and much faster than anyone thought possible! My deepest condolences to the victims of the terrible shooting in Douglas County @DCSheriff, and their families. We love our police and law enforcement  God Bless them all! #LESM As our Country rapidly grows stronger and smarter, I want to wish all of my friends, supporters, enemies, haters, and even the very dishonest Fake News Media, a Happy and Healthy New Year. 2018 will be a great year for America! What a year its been, and we're just getting started. Together, we are MAKING AMERICA GREAT AGAIN! Happy New Year!! WOW, @foxandfrlends  Dossier is bogus. Clinton Campaign, DNC funded Dossier. FBI CANNOT after all of this time VERIFY CLAIMS IN DOSSIER OF RUSSIA/TRUMP COLLUSION. FBI TAINTED. And they used this Crooked Hillary pile of garbage as the basis for going after the Trump Campaign! The train accident that just occurred in DuPont, WA shows more than ever why our soon to be submitted infrastructure plan must be approved quickly. Seven trillion dollars spent in the Middle East while our roads, bridges, tunnels, railways and more crumble! Not for long! Congressman Ron DeSantis is a brilliant young leader, Yale and then Harvard Law, who would make a GREAT Governor of Florida. He loves our Country and is a true FIGHTER! Was @foxandfriends just named the most influential show in news? You deserve it  three great people! The many Fake News Hate Shows should study your formula for success! Oppressive regimes cannot endure forever, and the day will come when the Iranian people will face a choice. The world is watching! The entire world understands that the good people of Iran want change, and, other than the vast military power of the United States, that Irans people\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('training_data.txt', 'r')\n",
    "text = f.read()\n",
    "text[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 1,\n",
       " 'e': 2,\n",
       " 't': 3,\n",
       " 'a': 4,\n",
       " 'o': 5,\n",
       " 'n': 6,\n",
       " 'i': 7,\n",
       " 'r': 8,\n",
       " 's': 9,\n",
       " 'l': 10,\n",
       " 'h': 11,\n",
       " 'd': 12,\n",
       " 'u': 13,\n",
       " 'm': 14,\n",
       " 'c': 15,\n",
       " 'g': 16,\n",
       " 'y': 17,\n",
       " 'p': 18,\n",
       " '.': 19,\n",
       " 'w': 20,\n",
       " 'f': 21,\n",
       " 'b': 22,\n",
       " 'A': 23,\n",
       " 'v': 24,\n",
       " ',': 25,\n",
       " 'k': 26,\n",
       " 'S': 27,\n",
       " 'T': 28,\n",
       " '!': 29,\n",
       " 'I': 30,\n",
       " 'C': 31,\n",
       " 'R': 32,\n",
       " 'N': 33,\n",
       " 'E': 34,\n",
       " 'M': 35,\n",
       " 'W': 36,\n",
       " 'O': 37,\n",
       " 'P': 38,\n",
       " 'F': 39,\n",
       " 'H': 40,\n",
       " 'G': 41,\n",
       " 'D': 42,\n",
       " 'B': 43,\n",
       " 'L': 44,\n",
       " 'U': 45,\n",
       " '@': 46,\n",
       " 'x': 47,\n",
       " '0': 48,\n",
       " '#': 49,\n",
       " 'J': 50,\n",
       " 'K': 51,\n",
       " 'j': 52,\n",
       " '\"': 53,\n",
       " \"'\": 54,\n",
       " 'V': 55,\n",
       " '1': 56,\n",
       " 'Y': 57,\n",
       " '2': 58,\n",
       " ':': 59,\n",
       " 'z': 60,\n",
       " '?': 61,\n",
       " '7': 62,\n",
       " '/': 63,\n",
       " '5': 64,\n",
       " '3': 65,\n",
       " '4': 66,\n",
       " '6': 67,\n",
       " 'q': 68,\n",
       " 'X': 69,\n",
       " '8': 70,\n",
       " '9': 71,\n",
       " 'Q': 72,\n",
       " 'Z': 73}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True, lower=False)\n",
    "tokenizer.fit_on_texts(text)\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 281143\n"
     ]
    }
   ],
   "source": [
    "max_id = len(tokenizer.word_index)\n",
    "dataset_size = tokenizer.document_count\n",
    "print(max_id, dataset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "[seq] = np.array(tokenizer.texts_to_sequences([text])) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([29,  7,  3,  5, 24,  0,  2, 10,  1,  0, 32, 12, 13, 21,  1,  7,  0,\n",
       "       36,  5,  1,  0, 26,  2,  3,  2,  1,  0,  4, 20,  0, 26, 17,  4,  5,\n",
       "        8,  4,  7,  1, 11,  0, 27,  1,  7,  7,  4,  7,  0, 19,  6,  2, 10,\n",
       "        0,  5, 12, 13,  1,  7,  4, 12,  8,  0, 23,  6,  4,  9,  3,  2,  6,\n",
       "        4,  5,  8,  0,  4, 20,  0, 39, 12, 13,  3,  5,  0, 31,  6, 15, 10,\n",
       "        2,  8,  0,  4, 14, 14, 12,  7,  7,  6,  5, 15,  0,  4,  5])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = .1\n",
    "seq = seq[:int(train_size * dataset_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, val = train_test_split(seq, test_size=0.1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28114"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)+len(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100\n",
    "window_length = n_steps + 1\n",
    "batch_size = 32\n",
    "\n",
    "def prepare_ds(ds):\n",
    "    # packages the list of tokens into data sets of length 101 \n",
    "    ds = ds.window(window_length, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda window: window.batch(window_length))\n",
    "    ds = ds.shuffle(dataset_size).batch(batch_size)\n",
    "    ds = ds.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "    ds = ds.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "    ds = ds.prefetch(1)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = prepare_ds(tf.data.Dataset.from_tensor_slices(train))\n",
    "dataset_val = prepare_ds(tf.data.Dataset.from_tensor_slices(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(32, 100, 73), dtype=float32, numpy=\n",
      "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 1., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [1., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 1., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[1., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [1., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [1., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)>, <tf.Tensor: shape=(32, 100), dtype=int64, numpy=\n",
      "array([[ 1,  7,  6, ..., 48, 43, 33],\n",
      "       [26, 22,  0, ...,  0, 17,  4],\n",
      "       [ 2, 10,  0, ..., 29, 26, 24],\n",
      "       ...,\n",
      "       [25,  5,  4, ...,  0, 17,  6],\n",
      "       [14,  3,  5, ..., 12,  9, 11],\n",
      "       [ 0,  3,  9, ...,  3, 17, 17]])>)\n"
     ]
    }
   ],
   "source": [
    "for i, element in enumerate(dataset_train):\n",
    "    if i >= 1:\n",
    "        break\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer gru_8 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer gru_8 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer gru_9 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer gru_9 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(100, return_sequences=True, input_shape=[None, max_id],\n",
    "                     dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.GRU(100, return_sequences=True,\n",
    "                     dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation=\"softmax\"))\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=2, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "788/788 [==============================] - 349s 443ms/step - loss: 2.5413 - accuracy: 0.3104 - val_loss: 2.1475 - val_accuracy: 0.4009\n",
      "Epoch 2/20\n",
      "788/788 [==============================] - 350s 445ms/step - loss: 1.9577 - accuracy: 0.4400 - val_loss: 1.9767 - val_accuracy: 0.4580\n",
      "Epoch 3/20\n",
      "788/788 [==============================] - 349s 443ms/step - loss: 1.7555 - accuracy: 0.4925 - val_loss: 1.9203 - val_accuracy: 0.4813\n",
      "Epoch 4/20\n",
      "788/788 [==============================] - 349s 443ms/step - loss: 1.6421 - accuracy: 0.5207 - val_loss: 1.9060 - val_accuracy: 0.4836\n",
      "Epoch 5/20\n",
      "788/788 [==============================] - 349s 443ms/step - loss: 1.5673 - accuracy: 0.5385 - val_loss: 1.8993 - val_accuracy: 0.4888\n",
      "Epoch 6/20\n",
      "788/788 [==============================] - 349s 443ms/step - loss: 1.5120 - accuracy: 0.5521 - val_loss: 1.9093 - val_accuracy: 0.4918\n",
      "Epoch 7/20\n",
      "788/788 [==============================] - 349s 443ms/step - loss: 1.4715 - accuracy: 0.5618 - val_loss: 1.9178 - val_accuracy: 0.4941\n",
      "Epoch 8/20\n",
      "788/788 [==============================] - 349s 443ms/step - loss: 1.4381 - accuracy: 0.5700 - val_loss: 1.9234 - val_accuracy: 0.5044\n",
      "Epoch 9/20\n",
      "788/788 [==============================] - 349s 443ms/step - loss: 1.4106 - accuracy: 0.5766 - val_loss: 1.9285 - val_accuracy: 0.5037\n",
      "Epoch 10/20\n",
      "788/788 [==============================] - 350s 445ms/step - loss: 1.3876 - accuracy: 0.5825 - val_loss: 1.9359 - val_accuracy: 0.5016\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset_train,\n",
    "                    epochs=20,\n",
    "                    callbacks=early_stopping_cb,\n",
    "                    validation_data=dataset_val\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    X = np.array(tokenizer.texts_to_sequences([text])) - 1\n",
    "    return tf.one_hot(X, max_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.000e-04, 2.000e-04, 8.000e-04, 9.961e-01, 7.000e-04, 1.000e-04,\n",
       "        4.000e-04, 0.000e+00, 2.000e-04, 1.000e-04, 4.000e-04, 0.000e+00,\n",
       "        1.000e-04, 0.000e+00, 1.000e-04, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        2.000e-04, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 1.000e-04, 0.000e+00, 0.000e+00, 1.000e-04, 0.000e+00,\n",
       "        1.000e-04, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = preprocess(\"Make Americ\")\n",
    "proba = model.predict(X_test)[0, -1:, :]\n",
    "np.around(proba, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred = np.argmax(model.predict(X_test), axis=-1)\n",
    "tokenizer.sequences_to_texts(Y_pred + 1)[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_char(text, temperature=1):\n",
    "    X_new = preprocess(text)\n",
    "    y_proba = model.predict(X_new)[0, -1:, :]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
    "    return tokenizer.sequences_to_texts(char_id.numpy())[0]\n",
    "\n",
    "\n",
    "def complete_text(text, n_chars=10, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'T TRUSTED NEWS! We are despent to the record and the record and the same and the same and the same and the record and the Fake News Media is '"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_text(\"T\", n_chars=140, temperature=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'T MAKE AMERICA GREAT AGAIN! LAST to the Iran most me this year for the very was my going of the Fake News Media is to the saying Harry and wh'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_text(\"T\", n_chars=140, temperature=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Time asked to, buch Americans comons. Merry McCabe, that is the @Whitej the encto be the people thouse in hesters whent themselves will be th'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_text(\"T\", n_chars=140, temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Take New Brks USA. I alsoeding wimer the dayncuration, a5 a1dastrek, you paoted hers, fartitial an Dor deeired talkh Korea. I hreod to forhyo'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_text(\"T\", n_chars=140, temperature=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.3 Python 3.7 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/tensorflow-2.3-gpu-py37-cu110-ubuntu18.04-v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
